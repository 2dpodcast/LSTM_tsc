{
  "name": "LSTM time-series classification",
  "tagline": "An LSTM for time-series classification",
  "body": "### LSTM for time-series classification\r\nThis post implements a Long Short-term memory for time series classification(LSTM). An LSTM is the extension of the classical Recurrent Neural Network. It has more flexibility and interpretable features such as a memory it can read, write and forget.\r\n\r\n## Context\r\nYou can find tutorials on LSTM's for sequence-to-sequence modelling. However, I haven't found any resources on sequence-to-label modelling. That ignited me to build an LSTM for time series classification in Tensorflow. This post is the result. \r\nPart of the inspiration springs from [this](http://www.cs.ucr.edu/~eamonn/time_series_data/) website. They offer a large database of time-series classification tasks for researchers to play with. Credits to them for making this database!\r\n\r\n## First stage\r\nThe implementation works on a self-made dataset containing sinusoids and a dataset from the [UCR archive](http://www.cs.ucr.edu/~eamonn/time_series_data/) named TwoPatterns. I heavily equipped the code with histograms, plots and visualizations to track the training procedure. I hope this enables you to get a feeling for the LSTM and to employ it on your own dataset.\r\nMoreover, I included many comments to explain the workflow of the code. I hope it also enables people unfamiliar to Tensorflow to start working with LSTM's.\r\n\r\n## Improvement\r\nThe code calls for many visualizations and histograms and contains many comment sections. That is a part of the improvement process, because the performance is way less than expected. In [this post](http://robromijnders.github.io/CNN_tsc/), I implement a CNN for the same dataset. It gets to 98% test-accuracy with a light-weight model. Our LSTM, however, reaches only 60% test-accuracy. I am working and learning hard to improve this model. I hope other people join me in this effort and I'd be glad to come into contact.\r\n\r\n### Results\r\nThe code is equipped with many TensorBoard diagrams, so you can follow your training:\r\n![TensorBoard_diagrams](https://github.com/RobRomijnders/LSTM_tsc/blob/master/progress/tb_for_github_io.png?raw=true)\r\nRunning the code without any changes will result in a diagram like this:\r\n![loss_acc_diagram](https://github.com/RobRomijnders/LSTM_tsc/blob/master/progress/loss_acc_for_github_io.png?raw=true)\r\nAnd after many iterations, it may look like this:\r\n![loss_acc_diagram_long](https://github.com/RobRomijnders/LSTM_tsc/blob/master/progress/160405_01.png?raw=true)\r\n\r\nCredits for this project go to [Tensorflow](https://www.tensorflow.org/versions/r0.7/tutorials/recurrent/index.html#recurrent-neural-networks) for providing a strong example, the [UCR archive](http://www.cs.ucr.edu/~eamonn/time_series_data/) for the dataset and my friend Ryan for strong feedback.\r\n\r\nAs always, I am curious to any comments and questions. Reach me at romijndersrob@gmail.com",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}